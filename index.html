<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EC2 LLM Instance Calculator</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .tooltip {
            position: relative;
            display: inline-block;
            cursor: pointer;
        }
        .tooltip .tooltip-text {
            visibility: hidden;
            width: 280px;
            background-color: #555;
            color: #fff;
            text-align: center;
            border-radius: 6px;
            padding: 10px;
            position: absolute;
            z-index: 10;
            bottom: 125%;
            left: 50%;
            margin-left: -140px;
            opacity: 0;
            transition: opacity 0.3s;
            font-size: 0.875rem;
            font-weight: 400;
        }
        .tooltip:hover .tooltip-text {
            visibility: visible;
            opacity: 1;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="container mx-auto p-4 md:p-8">
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold text-gray-900">EC2 LLM Instance Calculator</h1>
            <p class="mt-2 text-lg text-gray-600">Find the most cost-optimized AWS instance or cluster for your LLM workloads.</p>
        </header>

        <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
            <div class="lg:col-span-1 bg-white p-6 rounded-xl shadow-lg">
                <h2 class="text-2xl font-semibold mb-6 border-b pb-4">1. Configure Your Workload</h2>
                
                <div class="space-y-6">
                    <div>
                        <label for="task" class="block text-sm font-medium text-gray-700">Primary Task</label>
                        <select id="task" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                            <option value="inference">Inference</option>
                            <option value="training">Training</option>
                        </select>
                    </div>

                    <div>
                        <label for="modelSize" class="block text-sm font-medium text-gray-700">Model Size (Billion Parameters)</label>
                        <input type="number" id="modelSize" value="7" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                    </div>
                    
                    <div>
                        <label for="quantization" class="block text-sm font-medium text-gray-700">Precision / Quantization</label>
                        <select id="quantization" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                            <option value="2">FP16 / BF16 (2 bytes/param)</option>
                            <option value="1">INT8 (1 byte/param)</option>
                            <option value="0.5">INT4 (0.5 bytes/param)</option>
                            <option value="4">FP32 (4 bytes/param)</option>
                        </select>
                    </div>

                    <div id="parallelism-section">
                        <label for="parallelism" class="block text-sm font-medium text-gray-700 flex items-center">
                            Parallelism Strategy
                            <div class="tooltip ml-2">
                                <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 text-gray-400" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd" /></svg>
                                <span class="tooltip-text">Choose a strategy. 'Automatic' is best for most cases. Use others for specific needs like maximizing throughput (Data Parallelism) or fitting very large models (Tensor/Pipeline).</span>
                            </div>
                        </label>
                        <select id="parallelism" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                            <option value="automatic">Automatic (Recommended)</option>
                            <option value="data">Data Parallelism</option>
                            <option value="tensor_pipeline">Tensor + Pipeline Parallelism</option>
                        </select>
                    </div>

                    <div id="training-params" style="display: none;" class="space-y-6 mt-6">
                         <div>
                            <label for="trainingType" class="block text-sm font-medium text-gray-700">Training Type</label>
                            <select id="trainingType" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                                <option value="full">Full Fine-Tuning (Adam)</option>
                                <option value="peft">PEFT (e.g., LoRA)</option>
                            </select>
                        </div>
                    </div>

                    <div id="inference-params" class="space-y-6 mt-6">
                        <div>
                            <label for="contextLength" class="block text-sm font-medium text-gray-700">Context Length (Tokens)</label>
                            <input type="number" id="contextLength" value="4096" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                        </div>

                        <div>
                            <label for="batchSize" class="block text-sm font-medium text-gray-700">Concurrent Requests (Batch Size)</label>
                            <input type="number" id="batchSize" value="4" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2">
                        </div>
                    </div>
                </div>

                <div class="mt-8">
                    <button id="calculateBtn" class="w-full bg-indigo-600 text-white font-semibold py-3 px-4 rounded-md hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500 transition-colors duration-300">
                        Calculate & Find Instances
                    </button>
                </div>
            </div>

            <div class="lg:col-span-2">
                <div id="results-container" class="bg-white p-6 rounded-xl shadow-lg min-h-full">
                    <div id="summary-section"></div>
                    <div id="recommendations-section" class="mt-6"></div>
                    <div id="guidance-section" class="mt-6"></div>
                     <div id="disclaimer" class="mt-6 text-xs text-gray-500 italic">
                        <p><strong>Disclaimer:</strong> This is an estimation tool. Calculations are based on common heuristics. Instance data and pricing are based on the provided `us-east-2` (Ohio) documents and may change. Always perform your own testing and benchmarking before making a final decision.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        // --- DATA (VERIFIED & EXPANDED with pricing) ---
        const instanceData = [
            // P6
            { name: 'p6-b200.48xlarge', vcpu: 192, memory: 2048, gpuType: 'NVIDIA Blackwell', gpus: 8, gpuMemoryPerGpu: 180, totalGpuMemory: 1440, type: 'GPU', price: 113.9328 },
            // P5
            { name: 'p5.48xlarge', vcpu: 192, memory: 2048, gpuType: 'NVIDIA H100', gpus: 8, gpuMemoryPerGpu: 80, totalGpuMemory: 640, type: 'GPU', price: 55.0400 },
            // P4
            { name: 'p4d.24xlarge', vcpu: 96, memory: 1152, gpuType: 'NVIDIA A100', gpus: 8, gpuMemoryPerGpu: 40, totalGpuMemory: 320, type: 'GPU', price: 21.9576 },
            { name: 'p4de.24xlarge', vcpu: 96, memory: 1152, gpuType: 'NVIDIA A100', gpus: 8, gpuMemoryPerGpu: 80, totalGpuMemory: 640, type: 'GPU', price: 0 },
            // G6e
            { name: 'g6e.xlarge', vcpu: 4, memory: 32, gpuType: 'NVIDIA L40S', gpus: 1, gpuMemoryPerGpu: 48, totalGpuMemory: 48, type: 'GPU', price: 1.8610 },
            { name: 'g6e.2xlarge', vcpu: 8, memory: 64, gpuType: 'NVIDIA L40S', gpus: 1, gpuMemoryPerGpu: 48, totalGpuMemory: 48, type: 'GPU', price: 2.2420 },
            { name: 'g6e.4xlarge', vcpu: 16, memory: 128, gpuType: 'NVIDIA L40S', gpus: 1, gpuMemoryPerGpu: 48, totalGpuMemory: 48, type: 'GPU', price: 3.0042 },
            { name: 'g6e.8xlarge', vcpu: 32, memory: 256, gpuType: 'NVIDIA L40S', gpus: 1, gpuMemoryPerGpu: 48, totalGpuMemory: 48, type: 'GPU', price: 4.5285 },
            { name: 'g6e.16xlarge', vcpu: 64, memory: 512, gpuType: 'NVIDIA L40S', gpus: 1, gpuMemoryPerGpu: 48, totalGpuMemory: 48, type: 'GPU', price: 7.5771 },
            { name: 'g6e.12xlarge', vcpu: 48, memory: 384, gpuType: 'NVIDIA L40S', gpus: 4, gpuMemoryPerGpu: 48, totalGpuMemory: 192, type: 'GPU', price: 10.4926 },
            { name: 'g6e.24xlarge', vcpu: 96, memory: 768, gpuType: 'NVIDIA L40S', gpus: 4, gpuMemoryPerGpu: 48, totalGpuMemory: 192, type: 'GPU', price: 15.0655 },
            { name: 'g6e.48xlarge', vcpu: 192, memory: 1536, gpuType: 'NVIDIA L40S', gpus: 8, gpuMemoryPerGpu: 48, totalGpuMemory: 384, type: 'GPU', price: 30.1311 },
            // G6
            { name: 'g6.xlarge', vcpu: 4, memory: 16, gpuType: 'NVIDIA L4', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 0.8048 },
            { name: 'g6.2xlarge', vcpu: 8, memory: 32, gpuType: 'NVIDIA L4', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 0.9776 },
            { name: 'g6.4xlarge', vcpu: 16, memory: 64, gpuType: 'NVIDIA L4', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 1.3232 },
            { name: 'g6.8xlarge', vcpu: 32, memory: 128, gpuType: 'NVIDIA L4', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 2.0144 },
            { name: 'g6.16xlarge', vcpu: 64, memory: 256, gpuType: 'NVIDIA L4', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 3.3968 },
            { name: 'g6.12xlarge', vcpu: 48, memory: 192, gpuType: 'NVIDIA L4', gpus: 4, gpuMemoryPerGpu: 24, totalGpuMemory: 96, type: 'GPU', price: 4.6016 },
            { name: 'g6.24xlarge', vcpu: 96, memory: 384, gpuType: 'NVIDIA L4', gpus: 4, gpuMemoryPerGpu: 24, totalGpuMemory: 96, type: 'GPU', price: 6.6752 },
            { name: 'g6.48xlarge', vcpu: 192, memory: 768, gpuType: 'NVIDIA L4', gpus: 8, gpuMemoryPerGpu: 24, totalGpuMemory: 192, type: 'GPU', price: 13.3504 },
            // G5
            { name: 'g5.xlarge', vcpu: 4, memory: 16, gpuType: 'NVIDIA A10G', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 1.0060 },
            { name: 'g5.2xlarge', vcpu: 8, memory: 32, gpuType: 'NVIDIA A10G', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 1.2120 },
            { name: 'g5.4xlarge', vcpu: 16, memory: 64, gpuType: 'NVIDIA A10G', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 1.6240 },
            { name: 'g5.8xlarge', vcpu: 32, memory: 128, gpuType: 'NVIDIA A10G', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 2.4480 },
            { name: 'g5.16xlarge', vcpu: 64, memory: 256, gpuType: 'NVIDIA A10G', gpus: 1, gpuMemoryPerGpu: 24, totalGpuMemory: 24, type: 'GPU', price: 4.0960 },
            { name: 'g5.12xlarge', vcpu: 48, memory: 192, gpuType: 'NVIDIA A10G', gpus: 4, gpuMemoryPerGpu: 24, totalGpuMemory: 96, type: 'GPU', price: 5.6720 },
            { name: 'g5.24xlarge', vcpu: 96, memory: 384, gpuType: 'NVIDIA A10G', gpus: 4, gpuMemoryPerGpu: 24, totalGpuMemory: 96, type: 'GPU', price: 8.1440 },
            { name: 'g5.48xlarge', vcpu: 192, memory: 768, gpuType: 'NVIDIA A10G', gpus: 8, gpuMemoryPerGpu: 24, totalGpuMemory: 192, type: 'GPU', price: 16.2880 },
             // TRN2
            { name: 'trn2.48xlarge', vcpu: 192, memory: 2048, gpuType: 'Trainium2', gpus: 16, gpuMemoryPerGpu: 96, totalGpuMemory: 1536, type: 'Trainium', price: 0 },
            // TRN1
            { name: 'trn1.2xlarge', vcpu: 8, memory: 32, gpuType: 'Trainium', gpus: 1, gpuMemoryPerGpu: 32, totalGpuMemory: 32, type: 'Trainium', price: 1.3437 },
            { name: 'trn1.32xlarge', vcpu: 128, memory: 512, gpuType: 'Trainium', gpus: 16, gpuMemoryPerGpu: 32, totalGpuMemory: 512, type: 'Trainium', price: 21.5000 },
            { name: 'trn1n.32xlarge', vcpu: 128, memory: 512, gpuType: 'Trainium', gpus: 16, gpuMemoryPerGpu: 32, totalGpuMemory: 512, type: 'Trainium', price: 24.7800 },
            // INF2
            { name: 'inf2.xlarge', vcpu: 4, memory: 16, gpuType: 'Inferentia2', gpus: 1, gpuMemoryPerGpu: 32, totalGpuMemory: 32, type: 'Inferentia', price: 0.7582 },
            { name: 'inf2.8xlarge', vcpu: 32, memory: 128, gpuType: 'Inferentia2', gpus: 1, gpuMemoryPerGpu: 32, totalGpuMemory: 32, type: 'Inferentia', price: 1.9678 },
            { name: 'inf2.24xlarge', vcpu: 96, memory: 384, gpuType: 'Inferentia2', gpus: 6, gpuMemoryPerGpu: 32, totalGpuMemory: 192, type: 'Inferentia', price: 6.4906 },
            { name: 'inf2.48xlarge', vcpu: 192, memory: 768, gpuType: 'Inferentia2', gpus: 12, gpuMemoryPerGpu: 32, totalGpuMemory: 384, type: 'Inferentia', price: 12.9812 },
        ];


        // --- ELEMENTS ---
        const calculateBtn = document.getElementById('calculateBtn');
        const taskEl = document.getElementById('task');
        const modelSizeEl = document.getElementById('modelSize');
        const quantizationEl = document.getElementById('quantization');
        const contextLengthEl = document.getElementById('contextLength');
        const batchSizeEl = document.getElementById('batchSize');
        const summaryEl = document.getElementById('summary-section');
        const recommendationsEl = document.getElementById('recommendations-section');
        const guidanceEl = document.getElementById('guidance-section');
        const trainingParamsEl = document.getElementById('training-params');
        const inferenceParamsEl = document.getElementById('inference-params');
        const trainingTypeEl = document.getElementById('trainingType');
        const parallelismEl = document.getElementById('parallelism');

        // --- LOGIC ---
        function handleFormState() {
            const task = taskEl.value;
            trainingParamsEl.style.display = task === 'training' ? 'block' : 'none';
            inferenceParamsEl.style.display = task === 'inference' ? 'block' : 'none';
        }

        function calculateAndDisplay() {
            const task = taskEl.value;
            const modelSize = parseFloat(modelSizeEl.value) || 0;
            const quantizationBytes = parseFloat(quantizationEl.value) || 0;
            const parallelism = parallelismEl.value;
            
            summaryEl.innerHTML = '';
            recommendationsEl.innerHTML = '';
            guidanceEl.innerHTML = '';

            if (modelSize <= 0) {
                 summaryEl.innerHTML = `<p class="text-red-600 font-semibold p-4 bg-red-50 rounded-lg">Please enter a valid, positive model size.</p>`;
                 return;
            }
            
            let requiredMemory = 0;
            let calculationBreakdown = {};
            let finePrint = '';

            if (task === 'inference') {
                const contextLength = parseInt(contextLengthEl.value, 10) || 0;
                const batchSize = parseInt(batchSizeEl.value, 10) || 0;
                if (contextLength <= 0 || batchSize <= 0) {
                    summaryEl.innerHTML = `<p class="text-red-600 font-semibold p-4 bg-red-50 rounded-lg">Please enter valid context length and batch size for inference calculation.</p>`;
                    return;
                }
                const { memory, breakdown } = calculateInferenceMemory(modelSize, quantizationBytes, contextLength, batchSize);
                requiredMemory = memory;
                calculationBreakdown = breakdown;
                finePrint = `<strong>Inference Calculation:</strong> VRAM ≈ (Model Weights) + (KV Cache) + (Overhead).`;

            } else { // training
                const trainingType = trainingTypeEl.value;
                const { memory, breakdown, finePrint: fp } = calculateTrainingMemory(modelSize, quantizationBytes, trainingType);
                requiredMemory = memory;
                calculationBreakdown = breakdown;
                finePrint = fp;
            }
            
            renderSummary(requiredMemory, calculationBreakdown, finePrint);
            const recommendationType = findAndRenderInstances(requiredMemory, parallelism);
            renderGuidance(recommendationType);
        }

        function calculateInferenceMemory(modelSize, quantizationBytes, contextLength, batchSize) {
            const { layers, hiddenDim } = getModelInternals(modelSize);
            const weightsMemory = modelSize * quantizationBytes;
            const kvCachePerToken = layers * hiddenDim * 2 * quantizationBytes;
            const kvCacheMemory = (batchSize * contextLength * kvCachePerToken) / (1024**3);
            const overheadMemory = Math.max(2.0, weightsMemory * 0.1);
            const requiredMemory = weightsMemory + kvCacheMemory + overheadMemory;
            const breakdown = {
                "Model Weights": `${weightsMemory.toFixed(2)} GB`,
                "KV Cache": `${kvCacheMemory.toFixed(2)} GB`,
                "Framework Overhead": `${overheadMemory.toFixed(2)} GB`
            };
            return { memory: requiredMemory, breakdown };
        }
        
        function calculateTrainingMemory(modelSize, quantizationBytes, trainingType) {
            let requiredMemory = 0;
            let breakdown = {};
            let finePrint = '';
            
            if (trainingType === 'full') {
                const gradientBytes = 4;
                const adamStateBytes = 8;
                const totalMultiplier = quantizationBytes + gradientBytes + adamStateBytes;
                const memory = modelSize * totalMultiplier;
                const overhead = Math.max(4.0, memory * 0.1);
                requiredMemory = memory + overhead;
                
                breakdown = {
                    "Model Weights": `${(modelSize * quantizationBytes).toFixed(2)} GB`,
                    "Gradients (FP32)": `${(modelSize * gradientBytes).toFixed(2)} GB`,
                    "Adam States (FP32)": `${(modelSize * adamStateBytes).toFixed(2)} GB`,
                    "Framework Overhead": `${overhead.toFixed(2)} GB`,
                };
                finePrint = `<strong>Full-Tuning Calculation:</strong> VRAM ≈ Weights + Gradients + Optimizer States + Overhead.`;
            } else { // PEFT
                const weightsMemory = modelSize * quantizationBytes;
                const loraOverhead = weightsMemory * 0.15;
                const frameworkOverhead = 2.0;
                requiredMemory = weightsMemory + loraOverhead + frameworkOverhead;
                 breakdown = {
                    "Base Model Weights": `${weightsMemory.toFixed(2)} GB`,
                    "PEFT/Adapter Overhead": `${loraOverhead.toFixed(2)} GB`,
                    "Framework Overhead": `${frameworkOverhead.toFixed(2)} GB`,
                };
                finePrint = `<strong>PEFT Calculation:</strong> VRAM ≈ Weights + PEFT Adapters (estimated at 15% of model size) + Framework Overhead.`;
            }
            return { memory: requiredMemory, breakdown, finePrint };
        }

        function getModelInternals(paramsInBillions) {
            if (paramsInBillions <= 7) return { layers: 32, hiddenDim: 4096 };
            if (paramsInBillions <= 13) return { layers: 40, hiddenDim: 5120 };
            if (paramsInBillions <= 34) return { layers: 60, hiddenDim: 6656 };
            if (paramsInBillions <= 70) return { layers: 80, hiddenDim: 8192 };
            const layers = Math.round(80 + (paramsInBillions - 70) * 0.4); 
            const hiddenDim = Math.round(8192 + (paramsInBillions - 70) * 24);
            return { layers, hiddenDim };
        }

        function findAndRenderInstances(requiredMemory, parallelism) {
            let html = '<h2 class="text-xl font-semibold mb-4">3. Instance Recommendations</h2>';
            let recommendationType = 'none';

            if (parallelism === 'automatic') {
                const singleGpuFits = instanceData.filter(inst => inst.gpuMemoryPerGpu >= requiredMemory && inst.price > 0);
                if (singleGpuFits.length > 0) {
                    html += renderInstanceSection('Single Accelerator Instances', singleGpuFits, 'single');
                    recommendationType = 'single';
                }

                const multiGpuFits = instanceData.filter(inst => inst.gpus > 1 && inst.totalGpuMemory >= requiredMemory && inst.gpuMemoryPerGpu < requiredMemory && inst.price > 0);
                if (multiGpuFits.length > 0) {
                    html += renderInstanceSection('Multi-Accelerator Instances', multiGpuFits, 'multi');
                    recommendationType = recommendationType === 'single' ? 'single_and_multi' : 'multi';
                }

                if (recommendationType === 'none' && requiredMemory > 0) {
                    html += renderClusterSection('UltraCluster Recommendations (Multi-Node)', requiredMemory);
                    recommendationType = 'cluster';
                }
            } else if (parallelism === 'data') {
                const singleGpuFits = instanceData.filter(inst => inst.gpuMemoryPerGpu >= requiredMemory && inst.price > 0);
                if (singleGpuFits.length > 0) {
                     html += renderDataParallelismSection('Data Parallelism Recommendations', singleGpuFits);
                     recommendationType = 'data';
                }
            } else if (parallelism === 'tensor_pipeline') {
                const multiGpuFits = instanceData.filter(inst => inst.gpus > 1 && inst.totalGpuMemory >= requiredMemory && inst.price > 0);
                 if (multiGpuFits.length > 0) {
                    html += renderInstanceSection('Tensor/Pipeline Parallelism Recommendations', multiGpuFits, 'multi');
                    recommendationType = 'multi';
                } else if (requiredMemory > 0) {
                    html += renderClusterSection('UltraCluster Recommendations (Multi-Node)', requiredMemory);
                    recommendationType = 'cluster';
                }
            }
            
            if (recommendationType === 'none') {
                html += `<div class="text-center p-6 border-2 border-dashed rounded-lg">
                            <p class="font-semibold text-gray-700">No suitable instance found for ${requiredMemory.toFixed(2)} GB requirement and the selected parallelism strategy.</p>
                        </div>`;
            }

            recommendationsEl.innerHTML = html;
            return recommendationType;
        }

        // --- Rendering Logic ---
        function renderSummary(requiredMemory, breakdown, finePrint) {
            let breakdownHtml = Object.entries(breakdown).map(([key, value]) => `
                <li class="flex justify-between py-1">
                    <span class="text-gray-600">${key}:</span>
                    <span class="font-medium text-gray-800">${value}</span>
                </li>
            `).join('');

            summaryEl.innerHTML = `
                <h2 class="text-xl font-semibold mb-2">2. Estimated Memory Requirement</h2>
                <div class="p-4 bg-gray-100 rounded-lg">
                    <div class="flex justify-between items-baseline mb-3">
                        <h3 class="text-lg font-semibold">Total Needed:</h3>
                        <span class="text-2xl font-bold text-indigo-600">${requiredMemory.toFixed(2)} GB</span>
                    </div>
                    <ul class="text-sm space-y-1 border-t pt-2">
                        ${breakdownHtml}
                    </ul>
                    <p class="text-xs text-gray-500 mt-3 border-t pt-2">${finePrint}</p>
                </div>
            `;
        }
        
        function renderInstanceSection(title, instances, type) {
            const memoryKey = type === 'single' ? 'gpuMemoryPerGpu' : 'totalGpuMemory';
            
            const cheapest = [...instances].sort((a,b) => a.price - b.price)[0];
            const bestValue = [...instances].sort((a,b) => (a.price / a[memoryKey]) - (b.price / b[memoryKey]))[0];

            let instancesHtml = instances
                .sort((a,b) => a.price - b.price) 
                .slice(0, 5) 
                .map(inst => {
                
                const isCheapest = inst.name === cheapest.name;
                const isBestValue = inst.name === bestValue.name;
                
                let badgeHtml = '';
                if(isCheapest && isBestValue) {
                    badgeHtml = `<span class="text-xs font-semibold bg-emerald-100 text-emerald-800 px-2 py-1 rounded-full ml-3">Lowest Cost & Best Value</span>`;
                } else if (isCheapest) {
                    badgeHtml = `<span class="text-xs font-semibold bg-sky-100 text-sky-800 px-2 py-1 rounded-full ml-3">Lowest Cost</span>`;
                } else if (isBestValue) {
                     badgeHtml = `<span class="text-xs font-semibold bg-emerald-100 text-emerald-800 px-2 py-1 rounded-full ml-3">Best Value ($/GB)</span>`;
                }

                const colorClass = {
                    'GPU': 'bg-green-100 text-green-800',
                    'Inferentia': 'bg-purple-100 text-purple-800',
                    'Trainium': 'bg-yellow-100 text-yellow-800',
                }[inst.type];
                
                const memoryToShow = inst[memoryKey];
                const pricePerGb = inst.price / memoryToShow;

                return `
                    <div class="border rounded-lg p-3 transition-colors ${isCheapest || isBestValue ? 'border-indigo-500 bg-indigo-50' : 'border-gray-200 hover:bg-gray-50'}">
                        <div class="flex justify-between items-start">
                            <p class="font-semibold text-indigo-700 flex items-center">${inst.name} ${badgeHtml}</p>
                            <span class="text-base font-bold text-gray-700">$${inst.price.toFixed(4)}/hr</span>
                        </div>
                        <p class="text-sm text-gray-600 mt-1">
                            <span class="font-medium">${inst.gpus}x ${inst.gpuType}</span> | 
                            Accelerator Memory: <span class="font-medium">${memoryToShow} GB</span> | 
                            vCPUs: <span class="font-medium">${inst.vcpu}</span>
                        </p>
                         <p class="text-xs text-gray-500 mt-1">
                            Value: <span class="font-semibold text-gray-700">$${pricePerGb.toFixed(4)}</span> per GB | Type: <span class="font-semibold px-1 py-0.5 rounded ${colorClass}">${inst.type}</span>
                        </p>
                    </div>
                `;
            }).join('');
            
            return `
                <div class="mt-4">
                    <h3 class="text-lg font-semibold text-gray-800">${title}</h3>
                    <div class="mt-2 space-y-2">${instancesHtml}</div>
                </div>
            `;
        }

        function renderDataParallelismSection(title, instances) {
             const bestInstance = [...instances].sort((a,b) => (a.price / a.gpuMemoryPerGpu) - (b.price / b.gpuMemoryPerGpu))[0];

             let html = `
                <div class="border border-indigo-500 bg-indigo-50 rounded-lg p-3">
                    <div class="flex justify-between items-start">
                        <p class="font-semibold text-indigo-700 flex items-center">${bestInstance.name} (or similar)</p>
                         <span class="text-xs font-semibold bg-sky-100 text-sky-800 px-2 py-1 rounded-full ml-3">Recommended Node</span>
                    </div>
                     <p class="text-sm text-gray-600 mt-1">
                        Deploy multiple replicas of this instance to increase throughput. For example, deploying 4 nodes would handle 4x the concurrent requests for a total cost of <strong>$${(bestInstance.price * 4).toFixed(2)}/hr</strong>.
                    </p>
                </div>
             `;
             return `
                <div class="mt-4">
                    <h3 class="text-lg font-semibold text-gray-800">${title}</h3>
                    <div class="mt-2 space-y-2">${html}</div>
                </div>
            `;
        }

        function renderClusterSection(title, requiredMemory) {
             const clusterCandidates = instanceData.filter(inst => 
                ['p5.48xlarge', 'p6-b200.48xlarge', 'trn1n.32xlarge'].includes(inst.name) && inst.price > 0
            );

            const clusterConfigs = clusterCandidates.map(inst => {
                const nodesNeeded = Math.ceil(requiredMemory / inst.totalGpuMemory);
                return {
                    ...inst,
                    nodesNeeded: nodesNeeded,
                    totalClusterCost: nodesNeeded * inst.price,
                    totalClusterMemory: nodesNeeded * inst.totalGpuMemory,
                };
            }).sort((a,b) => a.totalClusterCost - b.totalClusterCost);

            let clusterHtml = clusterConfigs.map(config => {
                 const colorClass = { 'GPU': 'bg-green-100 text-green-800', 'Trainium': 'bg-yellow-100 text-yellow-800' }[config.type];
                return `
                     <div class="border border-indigo-500 bg-indigo-50 rounded-lg p-3">
                        <div class="flex justify-between items-start">
                            <p class="font-semibold text-indigo-700 flex items-center">${config.nodesNeeded} x ${config.name}</p>
                            <span class="text-base font-bold text-gray-700">$${config.totalClusterCost.toFixed(2)}/hr (Total)</span>
                        </div>
                        <p class="text-sm text-gray-600 mt-1">
                           Total Memory: <span class="font-medium">${config.totalClusterMemory.toLocaleString()} GB</span> | Type: <span class="font-semibold px-1 py-0.5 rounded ${colorClass}">${config.type}</span>
                        </p>
                    </div>
                `
            }).join('');
            
            return `
                <div class="mt-4">
                    <h3 class="text-lg font-semibold text-gray-800">${title}</h3>
                    <div class="mt-2 space-y-2">${clusterHtml}</div>
                </div>
            `;
        }


        function renderGuidance(recommendationType) {
            let parallelismHtml = '';
            if (recommendationType === 'multi' || recommendationType === 'single_and_multi') {
                parallelismHtml = `
                    <div class="p-3 bg-blue-50 rounded-lg">
                        <h4 class="font-bold text-blue-800">Recommended Strategy: Model Parallelism</h4>
                        <p class="text-blue-700">Your model is too large for a single accelerator but fits on a multi-accelerator instance. Use a combination of <strong class="font-semibold">Tensor Parallelism</strong> (splitting layers across accelerators) and <strong class="font-semibold">Pipeline Parallelism</strong> (splitting the whole model layer-by-layer) to distribute the workload.</p>
                    </div>`;
            } else if (recommendationType === 'cluster') {
                 parallelismHtml = `
                    <div class="p-3 bg-red-50 rounded-lg">
                        <h4 class="font-bold text-red-800">Recommended Strategy: Multi-Node Parallelism (FSDP)</h4>
                        <p class="text-red-700">Your model is too large for any single server. You will need a multi-node cluster. The recommended approach is <strong class="font-semibold">Fully Sharded Data Parallelism (FSDP)</strong>, which shards the model's weights, gradients, and optimizer states across all accelerators in the cluster. This is the most memory-efficient method for very large scale training.</p>
                    </div>`;
            }

            guidanceEl.innerHTML = `
                <h2 class="text-xl font-semibold mb-4 border-t pt-6">4. Fine Print & Guidance</h2>
                <div class="space-y-4 text-sm">
                    ${parallelismHtml}
                    <div class="p-3 bg-gray-100 rounded-lg">
                        <h4 class="font-bold text-gray-800">Parallelism Strategies Explained</h4>
                        <ul class="list-disc list-inside mt-2 space-y-2 text-gray-700">
                           <li><strong>Data Parallelism:</strong> Replicates the model across multiple accelerators. It's used to increase <span class="font-semibold">throughput</span> (e.g., more requests per second), not to fit larger models. Best for when a model already fits on a single accelerator.</li>
                            <li><strong>Tensor Parallelism:</strong> Splits a single layer of a model across multiple accelerators. This is necessary when the weights of a single layer are too large for one accelerator's memory.</li>
                             <li><strong>Pipeline Parallelism:</strong> Splits the entire model, assigning different layers to different accelerators. This is necessary when the full model is too deep to fit on one accelerator, even with Tensor Parallelism.</li>
                             <li><strong>Expert Parallelism:</strong> A special technique required only for Mixture-of-Experts (MoE) models to route data to the correct "expert" layers.</li>
                        </ul>
                    </div>
                </div>
            `;
        }
        
        // --- EVENT LISTENERS ---
        calculateBtn.addEventListener('click', calculateAndDisplay);
        taskEl.addEventListener('change', handleFormState);
        parallelismEl.addEventListener('change', calculateAndDisplay);


        // --- Initial Load ---
        window.onload = () => {
            handleFormState();
            calculateAndDisplay();
        };

    </script>
</body>
</html>
