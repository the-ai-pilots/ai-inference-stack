<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Modern AI Inference Stack: An Interactive Infographic</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F0F4F8; /* Light Blue-Gray */
            color: #1E293B; /* Slate 800 */
        }
        .bg-primary-dark { background-color: #0A2463; }
        .bg-primary-medium { background-color: #3E92CC; }
        .bg-primary-light { background-color: #D6E6F2; }
        .text-accent-light { color: #A5D8F3; }
        .text-accent-dark { color: #1E293B; }
        .border-accent { border-color: #3E92CC; }

        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 400px;
        }

        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
            }
        }
        
        .flow-arrow {
            position: relative;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .flow-arrow::after {
            content: '→';
            font-size: 2rem;
            color: #3E92CC;
            font-weight: bold;
            transform: translateY(-50%);
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
        }
        
        .flow-arrow-down::after {
            content: '↓';
        }
        
        .kpi-card {
            background-color: #ffffff;
            border-radius: 0.75rem;
            padding: 1.5rem;
            box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -4px rgb(0 0 0 / 0.1);
            border-left: 5px solid #3E92CC;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
        }

        .kpi-value {
            font-size: 3rem;
            font-weight: 900;
            color: #0A2463;
        }
        
        .kpi-label {
            font-size: 1rem;
            font-weight: 500;
            color: #475569; /* Slate 600 */
        }
        .gemini-btn {
            background-color: #3E92CC;
            color: white;
            font-weight: 600;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            transition: background-color 0.3s;
            cursor: pointer;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }
        .gemini-btn:hover {
            background-color: #0A2463;
        }
        .gemini-response {
            background-color: #ffffff;
            border: 1px solid #D6E6F2;
            padding: 1rem;
            margin-top: 1rem;
            border-radius: 0.5rem;
            white-space: pre-wrap;
            font-family: monospace;
        }
        .loader {
            width: 24px;
            height: 24px;
            border: 3px solid #FFF;
            border-bottom-color: transparent;
            border-radius: 50%;
            display: inline-block;
            box-sizing: border-box;
            animation: rotation 1s linear infinite;
        }
        @keyframes rotation {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="antialiased">
    <header class="bg-primary-dark text-white p-8 text-center shadow-lg">
        <h1 class="text-4xl md:text-5xl font-extrabold mb-2">The Modern AI Inference Stack</h1>
        <p class="text-lg md:text-xl text-accent-light max-w-4xl mx-auto">An interactive deep-dive into the engines, compilers, and strategies accelerating the new era of AI, from cloud to edge.</p>
    </header>

    <main class="container mx-auto p-4 md:p-8">
        
        <section id="intro" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-4">Inference: Where AI Delivers Value</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                While model training is a massive capital expense, inference is a continuous operational cost that defines user experience and economic viability. Optimizing inference isn't just about speed; it's a fundamental business necessity. The entire stack, from hardware to software, is being re-imagined to tackle this challenge.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6 text-center">
                <div class="kpi-card">
                    <div class="kpi-value">~80%</div>
                    <div class="kpi-label">of AI/ML compute costs can be from inference in production.</div>
                </div>
                <div class="kpi-card">
                     <span class="text-5xl font-black text-primary-medium">⚡</span>
                    <div class="kpi-value">Latency</div>
                    <div class="kpi-label">The critical metric for real-time user-facing applications.</div>
                </div>
                <div class="kpi-card">
                    <span class="text-5xl font-black text-primary-medium">📈</span>
                    <div class="kpi-value">Throughput</div>
                    <div class="kpi-label">The key to cost-efficiency and serving millions of users at scale.</div>
                </div>
            </div>
        </section>

        <section id="kv-cache" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-2">The KV Cache: LLM's Memory Bottleneck</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                Autoregressive generation in LLMs requires storing a "Key-Value Cache" for all previous tokens, which grows unpredictably and consumes enormous amounts of GPU memory. Traditional systems waste 60-80% of this memory due to fragmentation.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4 text-center">The Solution: PagedAttention (vLLM)</h3>
                    <p class="mb-4 text-center">Inspired by operating systems, PagedAttention treats the KV Cache as virtual memory, allocating it in non-contiguous "blocks" to virtually eliminate waste and enable efficient sharing.</p>
                    <div class="chart-container h-64 md:h-80"><canvas id="memoryWasteChart"></canvas></div>
                </div>
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4 text-center">From Fragmentation to Efficiency</h3>
                    <div class="space-y-4">
                        <div>
                            <h4 class="font-semibold mb-1">Traditional (Fragmented)</h4>
                            <div class="bg-primary-light p-2 rounded-lg flex space-x-1">
                                <div class="bg-red-300 h-8 rounded" style="width: 40%;"></div>
                                <div class="bg-gray-300 h-8 rounded" style="width: 15%;"></div>
                                <div class="bg-red-300 h-8 rounded" style="width: 30%;"></div>
                                <div class="bg-gray-300 h-8 rounded" style="width: 15%;"></div>
                            </div>
                            <p class="text-sm mt-1 text-gray-600">Large, contiguous blocks lead to wasted memory (gray areas).</p>
                        </div>
                         <div class="text-center text-4xl font-bold text-primary-medium">↓</div>
                        <div>
                            <h4 class="font-semibold mb-1">PagedAttention (Efficient)</h4>
                            <div class="bg-primary-light p-2 rounded-lg flex space-x-1">
                                <div class="bg-blue-400 h-8 rounded" style="width: 20%;"></div>
                                <div class="bg-blue-400 h-8 rounded" style="width: 20%;"></div>
                                <div class="bg-blue-400 h-8 rounded" style="width: 20%;"></div>
                                <div class="bg-blue-400 h-8 rounded" style="width: 20%;"></div>
                                <div class="bg-blue-400 h-8 rounded" style="width: 18%;"></div>
                                <div class="bg-gray-300 h-8 rounded" style="width: 2%;"></div>
                            </div>
                             <p class="text-sm mt-1 text-gray-600">Small, non-contiguous blocks minimize waste to <4%.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="engine-comparison" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-2">A Landscape of Leading Inference Engines</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                The modern inference landscape is dominated by specialized engines, each with unique innovations. While some, like vLLM and SGLang, optimize within the existing ecosystem, others like Modular and Apple are building vertically integrated stacks for maximum performance on their target platforms.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8">
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4 text-center">Engine Performance Highlights</h3>
                    <p class="mb-4 text-center">Different engines excel at different tasks. Benchmarks show SGLang's advantage in multi-turn chat, while vLLM is a throughput powerhouse for single-round inference. MAX and MLX show strong performance through full-stack integration.</p>
                    <div class="chart-container"><canvas id="enginePerfChart"></canvas></div>
                </div>
                 <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4 text-center">Latency Deep-Dive: TTFT vs. TPOT</h3>
                    <p class="mb-4 text-center">For interactive applications, two metrics matter: Time to First Token (TTFT) for perceived responsiveness, and Time Per Output Token (TPOT) for streaming speed. Engines optimize for both, but the balance is key.</p>
                    <div class="chart-container"><canvas id="latencyChart"></canvas></div>
                </div>
            </div>
            <div class="bg-white p-6 rounded-lg shadow-md overflow-x-auto">
                 <h3 class="text-xl font-bold mb-4 text-center">Feature Comparison</h3>
                <table class="w-full text-left border-collapse">
                    <thead>
                        <tr class="bg-primary-light">
                            <th class="p-3 font-semibold text-sm">Feature</th>
                            <th class="p-3 font-semibold text-sm">vLLM</th>
                            <th class="p-3 font-semibold text-sm">SGLang</th>
                            <th class="p-3 font-semibold text-sm">Modular MAX</th>
                            <th class="p-3 font-semibold text-sm">Apple MLX</th>
                        </tr>
                    </thead>
                    <tbody class="divide-y divide-gray-200">
                        <tr>
                            <td class="p-3 font-medium">Core Tech</td>
                            <td class="p-3">PagedAttention</td>
                            <td class="p-3">RadixAttention</td>
                            <td class="p-3">Mojo Language, MAX Engine</td>
                            <td class="p-3">Unified Memory, Lazy Compute</td>
                        </tr>
                        <tr>
                            <td class="p-3 font-medium">Best For</td>
                            <td class="p-3">High-throughput serving</td>
                            <td class="p-3">Complex apps (chat, agents)</td>
                            <td class="p-3">Unified, portable stack</td>
                            <td class="p-3">On-device Apple research</td>
                        </tr>
                        <tr>
                            <td class="p-3 font-medium">Parallelism</td>
                            <td class="p-3">Tensor, Pipeline</td>
                            <td class="p-3">Tensor, Pipeline, Data</td>
                            <td class="p-3">Multi-GPU support</td>
                            <td class="p-3">Single device focus</td>
                        </tr>
                        <tr>
                            <td class="p-3 font-medium">Ecosystem</td>
                            <td class="p-3">Python, Ray</td>
                            <td class="p-3">Python, Transformers</td>
                            <td class="p-3">Mojo, Python</td>
                            <td class="p-3">Python, Swift, C++</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="advanced-arch" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-2">Advanced Architectures: Rewriting the Rules of Execution</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                Beyond memory management, researchers are fundamentally rethinking the execution flow of LLMs to eliminate the final sources of latency and inefficiency: kernel launch overhead and workload interference.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="bg-white p-6 rounded-lg shadow-md text-center">
                    <h3 class="text-xl font-bold mb-4">The MegaKernel Paradigm</h3>
                    <p class="mb-4">Instead of thousands of small kernel launches, the entire model is compiled into a single, persistent GPU kernel, eliminating CPU overhead and enabling deep, fine-grained optimizations.</p>
                    <div class="flex flex-col items-center space-y-4">
                        <div class="p-4 rounded-lg bg-red-100 border border-red-300 w-full">
                            <h4 class="font-semibold">Conventional</h4>
                            <p class="text-sm">CPU ↔ GPU (1000s of launches)</p>
                        </div>
                        <div class="text-4xl font-bold text-primary-medium">↓</div>
                        <div class="p-4 rounded-lg bg-green-100 border border-green-300 w-full">
                            <h4 class="font-semibold">MegaKernel</h4>
                            <p class="text-sm">CPU → GPU (1 persistent kernel)</p>
                        </div>
                    </div>
                </div>
                <div class="bg-white p-6 rounded-lg shadow-md text-center">
                    <h3 class="text-xl font-bold mb-4">Prefill-Decode Disaggregation</h3>
                    <p class="mb-4">Computation-bound "prefill" and memory-bound "decode" tasks are sent to separate, specialized GPU pools, eliminating head-of-line blocking and improving system-wide latency.</p>
                     <div class="flex flex-col items-center space-y-4">
                        <div class="p-4 rounded-lg bg-red-100 border border-red-300 w-full">
                            <h4 class="font-semibold">Co-located (Blocked)</h4>
                            <div class="flex justify-center space-x-2"><div class="font-bold text-red-500">Long Prefill</div><div class="text-gray-400">Decode</div><div class="text-gray-400">Decode</div></div>
                        </div>
                        <div class="text-4xl font-bold text-primary-medium">↓</div>
                         <div class="p-4 rounded-lg bg-green-100 border border-green-300 w-full">
                            <h4 class="font-semibold">Disaggregated (Parallel)</h4>
                            <div class="flex justify-between"><div class="font-bold text-green-600">Prefill Engine</div><div class="font-bold text-green-600">Decode Engine</div></div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="ecosystem" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-2">The Compiler & Runtime Ecosystem</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                A rich ecosystem of compilers and runtimes bridges the gap between high-level Python code and low-level hardware execution, enabling interoperability and hardware-specific optimization.
            </p>
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-xl font-bold mb-4 text-center">Pathways to Performance</h3>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-center items-start">
                    <div class="p-4 border-2 border-dashed border-accent rounded-lg">
                        <h4 class="font-bold mb-2">PyTorch / TensorFlow</h4>
                        <p class="text-sm">High-level model definition</p>
                    </div>
                    <div class="flow-arrow flow-arrow-down md:flow-arrow-down-0"></div>
                     <div class="p-4 rounded-lg bg-primary-light">
                        <h4 class="font-bold mb-2">ONNX</h4>
                        <p class="text-sm">The "Lingua Franca" for interoperability</p>
                    </div>
                </div>
                <div class="flex justify-center my-4">
                     <div class="text-4xl font-bold text-primary-medium -mt-2">↓</div>
                </div>
                 <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-center">
                    <div class="p-4 border border-accent rounded-lg">
                        <h4 class="font-bold mb-2">PyTorch 2.x `torch.compile`</h4>
                        <p class="text-sm">Native JIT compilation for easy speedups.</p>
                    </div>
                     <div class="p-4 border border-accent rounded-lg">
                        <h4 class="font-bold mb-2">NVIDIA TensorRT</h4>
                        <p class="text-sm">Aggressive optimization for NVIDIA GPUs.</p>
                    </div>
                     <div class="p-4 border border-accent rounded-lg">
                        <h4 class="font-bold mb-2">Intel OpenVINO</h4>
                        <p class="text-sm">Optimized deployment on Intel CPUs, GPUs & NPUs.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="scaling" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-2">Scaling Inference: From One to Millions</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                Deploying at scale requires robust orchestration and serving frameworks. The choice depends on the desired balance of raw performance, programmability, and operational consistency.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4 text-center">Cloud vs. Bare-Metal Trade-offs</h3>
                     <p class="mb-4 text-center">The choice between hyperscale cloud and bare-metal infrastructure involves a fundamental trade-off between on-demand flexibility and raw, predictable performance.</p>
                    <div class="chart-container"><canvas id="deploymentChart"></canvas></div>
                    <div class="text-center mt-4">
                        <button id="explainDeploymentBtn" class="gemini-btn">✨ Explain these Trade-offs</button>
                    </div>
                    <div id="deploymentExplanation" class="gemini-response hidden"></div>
                </div>
                 <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4 text-center">Serving Framework Comparison</h3>
                     <p class="mb-4 text-center">Triton excels at raw throughput, Ray Serve offers Pythonic flexibility for complex apps, and KServe provides a seamless, Kubernetes-native deployment experience.</p>
                    <div class="chart-container"><canvas id="servingFrameworkChart"></canvas></div>
                    <div class="text-center mt-4">
                         <button id="explainServingBtn" class="gemini-btn">✨ Explain this Comparison</button>
                    </div>
                    <div id="servingExplanation" class="gemini-response hidden"></div>
                </div>
            </div>
        </section>

        <section id="future" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-2">Future Imperatives: Secure & Sustainable AI</h2>
             <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                As inference becomes ubiquitous, two non-functional requirements are becoming strategic imperatives: verifiable security through confidential computing and environmental sustainability through Green AI.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="kpi-card text-center">
                    <div class="text-6xl mb-4">🛡️</div>
                    <h3 class="text-2xl font-bold mb-2">Confidential Computing</h3>
                    <p class="text-base text-gray-600">Using hardware-based Trusted Execution Environments (TEEs) to protect sensitive data and proprietary models while they are being processed, even from the cloud provider.</p>
                </div>
                <div class="kpi-card text-center">
                    <div class="text-6xl mb-4">🌿</div>
                    <h3 class="text-2xl font-bold mb-2">Green AI</h3>
                    <p class="text-base text-gray-600">A movement to improve the energy efficiency of AI through hardware innovation, model optimization, and transparent benchmarking to reduce its environmental footprint.</p>
                </div>
            </div>
        </section>

        <section id="gemini-advisor" class="bg-white p-8 rounded-lg shadow-xl border-t-4 border-accent">
            <h2 class="text-3xl font-bold text-center mb-2">✨ Strategic AI Advisor</h2>
            <p class="text-lg text-center max-w-3xl mx-auto mb-8">
                Get a personalized inference stack recommendation. Tell us about your project, and our AI advisor will suggest a starting point based on the concepts in this infographic.
            </p>
            <div class="max-w-2xl mx-auto">
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                    <div>
                        <label for="appType" class="block text-sm font-medium text-gray-700 mb-1">Application Type</label>
                        <select id="appType" class="w-full p-2 border border-gray-300 rounded-md shadow-sm">
                            <option>Real-time User Chatbot</option>
                            <option>Offline Batch Processing</option>
                            <option>High-Throughput API Service</option>
                            <option>On-Device Mobile App</option>
                        </select>
                    </div>
                    <div>
                        <label for="priority" class="block text-sm font-medium text-gray-700 mb-1">What is your #1 priority?</label>
                        <select id="priority" class="w-full p-2 border border-gray-300 rounded-md shadow-sm">
                            <option>Lowest possible latency (TTFT)</option>
                            <option>Highest request throughput</option>
                            <option>Ease of deployment & management</option>
                            <option>Maximum development flexibility</option>
                        </select>
                    </div>
                </div>
                <div class="text-center">
                    <button id="getRecommendationBtn" class="gemini-btn text-lg">Get Recommendation</button>
                </div>
                <div id="recommendationResponse" class="gemini-response hidden mt-6"></div>
            </div>
        </section>

    </main>

    <footer class="bg-primary-dark text-white p-4 text-center mt-16">
        <p>Infographic created based on "The Modern AI Inference Stack" research report. June 2025.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const primaryColor = '#3E92CC';
            const secondaryColor = '#0A2463';
            const lightColor = '#D6E6F2';
            const accentColor = '#A5D8F3';

            const tooltipPlugin = {
                tooltip: {
                    callbacks: {
                        title: function(tooltipItems) {
                            const item = tooltipItems[0];
                            let label = item.chart.data.labels[item.dataIndex];
                            if (Array.isArray(label)) {
                              return label.join(' ');
                            } else {
                              return label;
                            }
                        }
                    }
                }
            };
            
            const wrapLabel = (label, maxLength = 16) => {
                if (label.length <= maxLength) return label;
                const words = label.split(' ');
                let lines = [];
                let currentLine = '';
                for (const word of words) {
                    if ((currentLine + ' ' + word).trim().length > maxLength) {
                        lines.push(currentLine.trim());
                        currentLine = word;
                    } else {
                        currentLine = (currentLine + ' ' + word).trim();
                    }
                }
                lines.push(currentLine.trim());
                return lines;
            };

            const memoryWasteChart = new Chart(document.getElementById('memoryWasteChart'), {
                type: 'doughnut', data: { labels: ['Wasted Memory (Traditional)', 'Useful Memory'], datasets: [{ data: [70, 30], backgroundColor: [ '#E57373', secondaryColor ], borderColor: '#ffffff', borderWidth: 4, }] }, options: { responsive: true, maintainAspectRatio: false, plugins: { legend: { position: 'top' }, tooltip: tooltipPlugin.tooltip, title: { display: true, text: 'Memory Waste: Traditional vs. PagedAttention' } }, cutout: '60%' }
            });
            const enginePerfChart = new Chart(document.getElementById('enginePerfChart'), {
                type: 'bar', data: { labels: [ wrapLabel('SGLang (Cache Hit Rate vs Baseline)'), wrapLabel('vLLM (Throughput vs HF)'), wrapLabel('Modular MAX (Speed vs vLLM)'), wrapLabel('Apple MLX (Speed vs PyTorch MPS)') ], datasets: [{ label: 'Relative Performance Improvement (Factor)', data: [4, 24, 1.12, 2.5], backgroundColor: [primaryColor, secondaryColor, primaryColor, secondaryColor], borderColor: [primaryColor, secondaryColor, primaryColor, secondaryColor], borderWidth: 1 }] }, options: { indexAxis: 'y', responsive: true, maintainAspectRatio: false, plugins: { legend: { display: false }, tooltip: tooltipPlugin.tooltip }, scales: { x: { beginAtZero: true, title: { display: true, text: 'Performance Factor (Higher is Better)' } } } }
            });
            const latencyChart = new Chart(document.getElementById('latencyChart'), {
                type: 'bar', data: { labels: ['Interactive Chat', 'Code Generation', 'Summarization'], datasets: [ { label: 'Time to First Token (ms)', data: [150, 400, 300], backgroundColor: primaryColor, }, { label: 'Time Per Output Token (ms)', data: [40, 35, 55], backgroundColor: secondaryColor, } ] }, options: { responsive: true, maintainAspectRatio: false, plugins: { legend: { position: 'top' }, tooltip: tooltipPlugin.tooltip, }, scales: { y: { beginAtZero: true, title: { display: true, text: 'Latency (ms)' } } } }
            });
            const deploymentChart = new Chart(document.getElementById('deploymentChart'), {
                type: 'bar', data: { labels: ['Performance', 'Flexibility', 'Management', 'Cost Model'], datasets: [ { label: 'Hyperscale Cloud', data: [7, 9, 8, 6], backgroundColor: primaryColor, }, { label: 'Bare-Metal', data: [10, 5, 4, 8], backgroundColor: secondaryColor, } ] }, options: { indexAxis: 'y', responsive: true, maintainAspectRatio: false, scales: { x: { stacked: true, max: 18, display: false, }, y: { stacked: true } }, plugins: { legend: { position: 'top' }, tooltip: { ...tooltipPlugin.tooltip, mode: 'index', intersect: false } } }
            });
            const servingFrameworkChart = new Chart(document.getElementById('servingFrameworkChart'), {
                type: 'radar', data: { labels: ['Raw Throughput', 'Programmability', 'K8s Native', 'Ease of Use', 'Flexibility'], datasets: [ { label: 'NVIDIA Triton', data: [10, 4, 7, 8, 6], fill: true, backgroundColor: 'rgba(62, 146, 204, 0.2)', borderColor: primaryColor, pointBackgroundColor: primaryColor, }, { label: 'Ray Serve', data: [7, 10, 5, 7, 10], fill: true, backgroundColor: 'rgba(10, 36, 99, 0.2)', borderColor: secondaryColor, pointBackgroundColor: secondaryColor, }, { label: 'KServe', data: [6, 6, 10, 6, 8], fill: true, backgroundColor: 'rgba(165, 216, 243, 0.4)', borderColor: accentColor, pointBackgroundColor: accentColor, } ] }, options: { responsive: true, maintainAspectRatio: false, plugins: { legend: { position: 'top' }, tooltip: tooltipPlugin.tooltip }, scales: { r: { angleLines: { display: true }, suggestedMin: 0, suggestedMax: 10, pointLabels: { font: { size: 11 } } } } }
            });

            // --- Gemini API Integration ---
            const apiKey = ""; 
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

            const callGemini = async (prompt, buttonEl, responseEl) => {
                const originalButtonText = buttonEl.innerHTML;
                buttonEl.innerHTML = '<span class="loader"></span> Generating...';
                buttonEl.disabled = true;
                responseEl.classList.remove('hidden');
                responseEl.textContent = 'Thinking...';

                try {
                    const payload = { contents: [{ role: "user", parts: [{ text: prompt }] }] };
                    const response = await fetch(apiUrl, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(payload)
                    });

                    if (!response.ok) {
                        throw new Error(`API call failed with status: ${response.status}`);
                    }

                    const result = await response.json();
                    
                    if (result.candidates && result.candidates.length > 0) {
                        const text = result.candidates[0].content.parts[0].text;
                        responseEl.textContent = text;
                    } else {
                        throw new Error("No content received from API.");
                    }

                } catch (error) {
                    console.error("Gemini API Error:", error);
                    responseEl.textContent = `Error: Could not retrieve explanation. ${error.message}`;
                } finally {
                    buttonEl.innerHTML = originalButtonText;
                    buttonEl.disabled = false;
                }
            };
            
            document.getElementById('explainDeploymentBtn').addEventListener('click', (e) => {
                const prompt = `You are an expert AI systems architect. Explain the "Cloud vs. Bare-Metal Trade-offs" chart for an audience of technical managers.
                The chart data is as follows:
                - Hyperscale Cloud scores: Performance=7, Flexibility=9, Management=8, Cost Model=6.
                - Bare-Metal scores: Performance=10, Flexibility=5, Management=4, Cost Model=8.
                A higher score is better in that category.
                Explain the key takeaways. For example, why does bare-metal have higher performance but lower flexibility? Why is the cost model for cloud lower-rated? Keep the explanation concise and focused on actionable insights.`;
                callGemini(prompt, e.target, document.getElementById('deploymentExplanation'));
            });

            document.getElementById('explainServingBtn').addEventListener('click', (e) => {
                const prompt = `You are an expert AI systems architect. Explain the "Serving Framework Comparison" radar chart for an audience of DevOps engineers.
                The chart compares NVIDIA Triton, Ray Serve, and KServe on five axes: Raw Throughput, Programmability, K8s Native, Ease of Use, Flexibility.
                The data is:
                - Triton: [10, 4, 7, 8, 6]
                - Ray Serve: [7, 10, 5, 7, 10]
                - KServe: [6, 6, 10, 6, 8]
                Explain when a team should choose each framework based on these strengths and weaknesses. For example, 'Choose Triton if your top priority is...' or 'Consider Ray Serve when you need...'. Format the response with clear headings for each framework.`;
                callGemini(prompt, e.target, document.getElementById('servingExplanation'));
            });

            document.getElementById('getRecommendationBtn').addEventListener('click', (e) => {
                const appType = document.getElementById('appType').value;
                const priority = document.getElementById('priority').value;
                
                const infographicContext = `
                Key Inference Engines:
                - vLLM: Uses PagedAttention for high-throughput serving.
                - SGLang: Uses RadixAttention, excels at complex apps like multi-turn chat and agents.
                - Modular MAX: A new, unified stack with the Mojo language, aiming for portability.
                - Apple MLX: For on-device performance on Apple Silicon.

                Serving Frameworks:
                - NVIDIA Triton: Best for raw throughput.
                - Ray Serve: Highly programmable and flexible, great for complex Python logic.
                - KServe: Best for Kubernetes-native integration and management.
                `;

                const prompt = `You are the "Strategic AI Advisor" for an infographic on the modern AI inference stack.
                Based on the following context about available technologies:
                ${infographicContext}

                Provide a personalized stack recommendation for a user with the following requirements:
                - Application Type: ${appType}
                - #1 Priority: ${priority}

                Your response should have two parts:
                1. **Recommendation:** Clearly state the recommended Inference Engine and Serving Framework.
                2. **Justification:** Explain WHY this combination is the best fit for the user's specific needs, directly referencing their application type and priority.
                Keep the tone helpful and expert.`;
                
                callGemini(prompt, e.target, document.getElementById('recommendationResponse'));
            });
        });
    </script>
</body>
</html>
